= Writing a Commodore Component

Commodore components are bundles which contain templates and a component
class describing a component (for example, a fully-configured Crossplane stack) for
consumption by Kapitan. Additionally a Commodore component can contain
postprocessing filters, which are arbitrary Jsonnet scripts that are executed
by Commodore after Kapitan has compiled the component.

== Quickstart

To kickstart developing a component, Commodore provides the command
`new-component`. Prepare your working directory by fetching the Commodore
environment for some cluster by running

[source,bash]
--
pipenv run commodore compile <customer> <cluster>
--

Then you can bootstrap the repository for your new component by running

[source,bash]
--
pipenv run commodore new-component <component-name>
--

Now, you can start developing your component by writing Jsonnet in
`component/main.jsonnet`.

If you don't require any postprocessing of the Kapitan output, you can delete
the whole `postprocess` folder in the component repository. Removing the
folder will make Commodore skip the postprocessing step for the component
completely.

== The component defaults

Each component can define default configuration values for the Kapitan
inventory. Those default values should be configured in `class/defaults.yml`
which is inserted into the Kapitan class hierarchy at the lowest priority.

The component template creates a bare-bones `defaults.yml` which defines a
parameter to configure the namespace into which the component should be
deployed. For example for a component with the name `mycomponent`:

[source,yaml]
--
parameters:
  mycomponent:
    namespace: syn-mycomponent
--

=== The Kapitan class hierarchy

// TODO: link to Platform Configuration Managment SDD once they're open-sourced
The class hierarchy is documented in more detail in the Platform Configuration
Management SDD, but the brief outline is that the Kapitan target includes a
few well-defined classes based on the target cluster.

The hierarchy is as follows (starting at the lowest precedence):

* Component defaults, included as `defaults.<component-name>` for each component
* Global defaults, `global.common`
* Kubernetes distribution defaults, `global.distribution.<kubernetes-distribution>`. The
  intention of the hierarchy is that component classes are included in each
  distribution class separately to allow bootstrapping of new distributions in
  a multi-distribution environment easier
* Cloud provider defaults, `global.cloud.<cloud-provider>`
* Cloud provider region defaults, `global.cloud.<cloud-provider>.<region>`
* Cluster configuration, `<customer>.<cluster>`. The cluster configuration is
  managed in the customer's configuration repository. The customer repository
  can have any structure, the only requirement is that it contains a Kapitan
  class for each of the customer's clusters.

The intention for having the component defaults and class split into two
Kapitan classes is that components can be included at any point in the
hierarchy, without having their defaults overwrite defaults defined higher up
in the configuration hierarchy. F.e. without the split, a component included
for a particular cloud provider would overwrite configuration values specified
in `global.common` or `global.distribution.<kubernetes-distribution>` for which the
component provides defaults.

== The component templates

The component templates can be any templating language that Kapitan can
handle. Currently Commodore supports Jsonnet, Jinja2, Kadet (alpha) and Helm
(alpha) as templating languages. This guide will use Jsonnet as the
templating language for any examples.

Component templates can be stored anywhere in the Commodore component
repository, as long as they're correctly referenced by the component class.

From a template, the [Kapitan Inventory](https://kapitan.dev/inventory/) --
which is managed by Commodore -- can be accessed with

[source,jsonnet]
--
local kap = import "lib/kapitan.libjsonnet";
local inv = kap.inventory();
--

Any variables which are configured in any class in the inventory under
`parameters` can then be retrieved via

[source,jsonnet]
--
inv.parameters.section.subsection.value
--

Commodore ensures that Bitnami Labs's
[kube-libsonnet](https://github.com/bitnami-labs/kube-libsonnet)
is available as `lib/kube.libjsonnet`. This allows templates to reuse the
provided methods to abstract away a lot of the tedious bits of writing
Kubernetes objects.

As an illustration, below is an example `nginx` deployment which is written
using `kube-libsonnet`.

[source,jsonnet]
--
local kube = import 'lib/kube.libjsonnet';

local deployment = kube.Deployment('test-nginx') {
  spec+: {
    template+: {
      spec+: {
        containers_+: {
          default: kube.Container('nginx') {
            image: 'nginx',
            ports_+: { http: { containerPort: 80 } }
          }
        }
      }
    }
  }
}
--

=== Validating inventory schemas

Kapitan includes [JSON Schema](https://json-schema.org/), which can be used to
validate inventory section structures against schemas. To write schemas,
please refer to ["Understanding JSON Schema"](https://json-schema.org/understanding-json-schema/index.html).

Given the schema

[source,jsonnet]
--
local section_schema = {
  type: "object",
  properties: {
    key1: { type: "string" },
    key2: { type: "int" },
    key3: { type: "string", pattern: "^prefix-[0-9]+$" }
  },
  required: [ 'key1', 'key2' ]
};
--

and the example inventory

[source,yaml]
--
parameters:
  section_a:
    key1: test
    key2: 20
  section_b:
    key1: test
    key2: 20
    key3: prefix-0000
  section_c:
    key1: test
    key2: 50G
  section_d:
    key1: test
    key2: 20
    key3: other-2000
  section_e:
    key1: test
    key3: prefix-2000
--

we can validate the structure of each of `section_a`, `section_b` and
`section_c` using the `jsonschema()` function:

[source,jsonnet]
--
local validation = kap.jsonschema(inv.parameters.section_X, section_schema);
assert validation.valid: validation.reason;
--

Validation of `section_a` and `section_b` succeeds and produces no output.

Validation of `section_c` fails with:

[source]
--
Jsonnet error: failed to compile schema_example.jsonnet:
 RUNTIME ERROR: '50G' isn't of type 'integer'

Failed validating 'type' in schema['properties']['key2']:
    {'type': 'integer'}

On instance['key2']:
    '50G'
--

Validation of `section_d` fails with:

[source]
--
Jsonnet error: failed to compile schema_example.jsonnet:
 RUNTIME ERROR: 'other-2000' does not match '^prefix-[0-9]+$'

Failed validating 'pattern' in schema['properties']['key3']:
    {'pattern': '^prefix-[0-9]+$', 'type': 'string'}

On instance['key3']:
    'other-2000'
--

Validation of `section_e` fails with:

[source]
--
Jsonnet error: failed to compile schema_example.jsonnet:
 RUNTIME ERROR: 'key2' is a required property

Failed validating 'required' in schema:
    {'properties': {'key1': {'type': 'string'},
                    'key2': {'type': 'integer'},
                    'key3': {'pattern': '^prefix-[0-9]+$',
                             'type': 'string'}},
     'required': ['key1', 'key2'],
     'type': 'object'}

On instance:
    {'key1': 'test', 'key3': 'prefix-2000'}
--

If `validation.valid` isn't true, the `assert` will fail, which aborts the
compilation, and the reason for the validation failure will be displayed.

== The component class

Commodore looks for the component class in `class/<component-name>.yml`. Since
Kapitan does only process files in the inventory which end with `.yml`, it's
important that the component class is named exactly as specified.

The component class provides Kapitan with the information that's necessary to
compile a component.

Commodore components will always be stored under
`dependencies/<component-name>` in Kapitan's working directory. Commodore
configures Kapitan to look for inputs in the working directory and in
`dependencies`. To ensure that template file names can't cause conflicts
between different components, the component class will always have to specify
inputs in the form `<component-name>/path/to/the/input.jsonnet`, the component
class will always have to specify inputs in the form
`<component-name>/path/to/the/input.jsonnet`. For example:

[source,yaml]
--
parameters:
  kapitan:
    compile:
      - output_path: crossplane
        input_type: jsonnet
        output_type: yaml
        input_paths:
          - crossplane/component/main.jsonnet
--

To avoid name collisions in the output, each component should specify the
output path as the component's name for all compile instructions.

=== Rendering Helm charts with Kapitan

See [Kapitan's documentation](https://kapitan.dev/compile/#helm).

It's strongly suggested that each component downloads helm charts into
`dependencies/<component-name>` to avoid weird interactions if multiple
components build upon the same helm chart.

== Postprocessing filters

Postprocessing filters are defined in `postprocess/filters.yml`, which is
inspired by the Kapitan compile instructions. Commodore supports two different
filter types, `jsonnet` and `builtin`. Filters in other templating languages
aren't supported at the moment.

Filters of type `jsonnet` can be arbitrary Jsonnet. The format of the Jsonnet
is inspired by Kapitan and the postprocessor expects that each filter outputs
a JSON object where the keys are used as the name of the resulting output
files. For each file, the value of the object's key is rendered as YAML in
that file.

Builtin filters provide often-used filter actions to components. Currently,
Commodore provides only one builtin filter, `helm_namespace`. This filter
processes the output of a rendered Helm chart and adds a `metadata.namespace`
field to each object in the output. Additionally, if `create_namespace` is set
to the string `"true"`, the namespace itself is also created.
Builtin filters can take arguments in `filterargs`. Values in `filterargs` can
use Kapitan-style inventory references.

A sample `postprocess/filters.yml` might look like

[source,yaml]
--
filters:
  # The builtin helm_namespace filter takes a filter argument `namespace` and an optional argument `create_namespace` to create the namespace object.
  - path: crossplane/01_helmchart/crossplane/templates
    type: builtin
    filter: helm_namespace
    filterargs:
      namespace: ${crossplane:namespace}
      create_namespace: "true"
  # A fictional custom filter which adds some custom annotations to the Helm
  # chart output
  - output_path: crossplane/01_helmchart/crossplane/templates
    type: jsonnet
    filter add_monitoring_annotations_to_deployments.jsonnet
--

=== Available built-in filters

Builtin filters expect the argument `path` to indicate on which path in the
compiled Kapitan output they operate. This differs from custom filters which
have a parameter `output_path` indicating where to write the filter output.

* `helm_namespace`: Takes one argument `namespace` which is inserted as
  `.metadata.namespace` into all objects found in files that are stored in
  `output_path`.

=== Writing a custom postprocessing filter

Commodore provides a `commodore.libjsonnet` Jsonnet library which can be used
by Jsonnet filters to access the Kapitan inventory and to load YAML files:

[source,jsonnet]
--
local commodore = import 'lib/commodore.libjsonnet';
local inv = commodore.inventory();
--

The `inventory` function returns an object that behaves identically to the
object returned from `kapitan.libjsonnet`'s `inventory` function.

Additionally, each Jsonnet filter is executed with external variables
`component` and `target` set to the name of the component to which the filter
belongs and the name of the Kapitan compilation target respectively.

Commodore also provides `yaml_load` as a native callback to Jsonnet. This
allows filters to read in YAML files:

[source,jsonnet]
--
local object = commodore.yaml_load('/path/to/input.yaml');
--

The value of each key of the Jsonnet output object is dumped as YAML to
`compiled/target/<output_path>/<key>.yaml`. Filter authors can decide
themselves whether to write filters that overwrite their inputs, or not.

== Tips and hints

=== Converting existing YAML manifests
When writing components you usually already have working kubernetes yaml
manifests. Using a YAML to JSON function of your editor (VS Code) greatly
helps to speed up this process.

Also a lot of the existing YAML manifest is usually no longer needed
when using [kube-libsonnet](https://github.com/bitnami-labs/kube-libsonnet).

A good workflow could be:
1. copy paste your yaml snippet into a `kube.<object>` block
2. mark the the yaml snippet and convert to JSON using your editors plugin
3. remove everything that's done by kube-libsonnet
4. change remaining keys from `keyname:` to `keyname+:` for merging

==== Example
Existing YAML:
[source,yaml]
--
apiVersion: v1
kind: Namespace
metadata:
  name: test
  labels:
    a-label: test
--

How it should look in jsonnet:

[source,jsonnet]
--
{
  '00_namespace': kube.Namespace('test') {
    metadata+: {
        labels: {
            "a-label": "test"
        }
    }
  },
}
--
